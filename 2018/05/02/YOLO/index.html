<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="专注于机器学习、深度学习、图形图像处理。"><title>YOLO | 噢噢噢噢奥利</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">YOLO</h1><a id="logo" href="/.">噢噢噢噢奥利</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Start</i></a><a href="/archives/"><i class="fa fa-archive"> Archiv</i></a><a href="/about/"><i class="fa fa-user"> Über</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">YOLO</h1><div class="post-content"><p>&emsp;&emsp;YOLO（You Only Look Once）模型把目标检测看成一个回归问题，模型只有一个神经网络，可以直接输出bounding box和class probabilities，并且可以端到端的优化。<a id="more"></a></p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>YOLO模型进行目标检测的过程如下：<br><img src="https://i.loli.net/2018/05/07/5aefc75372353.png" alt=""><br>&emsp;&emsp;1. Resize image：把原始图片缩放到$448\times 448$大小。<br>&emsp;&emsp;2. Run convolutional network：把缩放后的图片送入网络。<br>&emsp;&emsp;3. Non-max suppression：非极大值抑制得到检测结果。</p>
<p>YOLO模型有一下优缺点：<br>&emsp;&emsp;1. 速度快。<br>&emsp;&emsp;2. YOLO模型能够看到完整图片，所以模型能隐式的学习关于类别和位置的上下文信息，不容易false positive，把背景识别为某个正类。相应的，R-CNN模型中，分类和候选框修正网络只能看到候选框的部分，容易把背景识别为前景。<br>&emsp;&emsp;3. YOLO能学习到对目标更一般的特征表示。主要是说把YOLO运用在新的领域表现更好。<br>&emsp;&emsp;4. YOLO网络在准确率上没有达到state-of-the-art的水平。定位精度不高，对小目标检测的效果不好。</p>
<h2 id="2-网络"><a href="#2-网络" class="headerlink" title="2. 网络"></a>2. 网络</h2><p>&emsp;&emsp;YOLO把输入图片分成$S\times S$个网格(grid cell)，如果一个网格是某个目标的中心，这个网格就负责检测这个目标。</p>
<p>&emsp;&emsp;每个网格预测$B$个边框（bounding box）和对应的置信度（confidence score）。某个边框的置信度定义如下：<br>$$<br>confidence = Pr(Object) \times IOU^{truth}_{pred}<br>$$</p>
<p>$$<br>Pr(Object) = \begin{cases}<br>0 &amp; cell中不包含任何目标    \\<br>1 &amp; otherwise<br>\end{cases}<br>$$<br>置信度包含两个方面：第一项$Pr(Object)$描述模型有多大的信心认为这个box包含某个目标，第二项$IOU$描述边框对ground truth的准确率。</p>
<p>&emsp;&emsp;每一个bounding box包含5个参数：$x,y,w,h,confidence$。$(x,y)$表示边框的中心，$(w,h)$表示边框的宽高，这四个值进行了归一化，通过归一化，这四个参数都在[0,1]之间，具体如下：<br>$$<br>(x,y)= \frac{边框中心相对于对应的cell左上角的像素点的偏移量}{cell的宽或高}<br>$$<br>$$<br>(w,h) = \frac{边框的宽或高}{resized图片的宽或高}<br>$$</p>
<p>&emsp;&emsp;每个网格预测C个条件概率$Pr(Class_i|Object)$，表示在这个网格下每一个类别的概率。不管一个网格预测多少边框，YOLO模型都只预测一组类别概率分布。</p>
<p>&emsp;&emsp;在测试时，使用边框的confidence和cell的class probabilities的乘积作为选择候选框的标准，称为边框的类别置信度：<br>$$<br>Pr(Class_i|Object)\times Pr(Object)\times IOU^{truth}_{pred} = Pr(Class_i)\times IOU^{truth}_{pred}<br>$$<br>这样，对于每个边框，都对应一组class confidence score，每个类别置信度表示：1）这个类别有多大概率出现在这个边框中，或者说边框属于这个类别的大小。2）预测的这个边框和ground truth的IOU，和GT的匹配程度，原文：how well the predicted box fits the objet。</p>
<p><img src="https://i.loli.net/2018/05/07/5aefc7539c4a9.png" alt=""></p>
<p>&emsp;&emsp;整个模型的输入为resized的图片，输出是$S\times S \times (B\times 5 + C)$的向量，这个向量表示了所有上文介绍的信息。作者在PASCAL VOC数据集上，设定$S=7,B=2$。</p>
<h3 id="2-1-网络结构"><a href="#2-1-网络结构" class="headerlink" title="2.1 网络结构"></a>2.1 网络结构</h3><p>&emsp;&emsp;网络结构就是卷积层做特征提取，全连接层预测需要的值：24层卷积，2层全连接。作者借鉴了GoogLeNet的做法，但是没有扩展宽度，每个模块就是简单的$1\times 1$卷积降维+$3\times 3$卷积特征提取。具体结构如下图：</p>
<p><img src="https://i.loli.net/2018/05/07/5aefc7536ff90.png" alt=""></p>
<h3 id="2-2-训练"><a href="#2-2-训练" class="headerlink" title="2.2 训练"></a>2.2 训练</h3><p><u>如何定义损失函数？</u><br>&emsp;&emsp;作者直接采用了平方损失，因为容易优化，但是目标是最大化mAP，所以平方损失会有点问题：<br>&emsp;&emsp;1）分类误差和定位误差不应该同等重要。<br>&emsp;&emsp;2）对于不包含目标的cell，Pr(Object) = 0，对应B个box的confidence = 0；对于包含目标的cell，对应box的confidence = IOU $\in (0,1]$；反向传播的时候，$(\hat {confidence} - 0)^2$项要比$(\hat {confidence} - IOU)^2$项对梯度的贡献大，这可能导致模型不稳定。我们更想要关注那些包含目标的cell。（个人理解是这样的，看其他博客解释比较含糊。）<br>&emsp;&emsp;3）采用均方误差，其同等对待大小不同的边界框，但是和较大的边框相比，模型应该对<strong>较小边框的坐标误差</strong>更敏感。</p>
<p>&emsp;&emsp;对于上面三个问题：<br>&emsp;&emsp;1）提高定位误差的系数，$\lambda_{coord}=5$，分类误差系数取1。<br>&emsp;&emsp;2）对于不包含任何目标的cell，这些cell对应的box，IOU误差系数取$\lambda_{noobj}=0.5$。对于包含目标的cell，这些cell对应的box，IOU误差系数取1。<br>&emsp;&emsp;3）预测$\sqrt w,\sqrt h$，而不是直接预测$w,h$，可以缓解第三个问题。</p>
<p>&emsp;&emsp;YOLO模型中，一个网格预测一组类别分布，也就只对应一个类别，但一个网格却预测多个边框。所以在训练的时候，对于每个目标，作者只想要一个边框去预测。在预测的边框中（作者意思应该是，在当前迭代这一步，输出的边框中），与目标Ground Truth的IOU最大的边框<strong>负责</strong>预测这个目标。对于不存在对应目标的边框，不计算坐标损失，也就是说，即使有N个网格包含了同一个目标，是有NB-1个边框没有Ground Truth的，是不计算坐标损失的。（文章开头说，目标中心对应的网格负责预测这个目标，能保证迭代中与GT的IOU最大的边框对应的网格就是目标中心对应的网格吗？？？）</p>
<p>&emsp;&emsp;总的损失函数是<strong>坐标误差+置信度误差+分类误差</strong>：<br>$$<br>\lambda_{coord} \sum_{i=1}^{S^2} \sum_{j=1}^{B} 1_{ij}^{obj}<br>[(x_i - \hat {x_i})^2+(y_i - \hat {y_i})^2]<br>+\lambda_{coord} \sum_{i=1}^{S^2} \sum_{j=1}^{B} 1_{ij}^{obj}<br>[(\sqrt w_i - {\sqrt{\hat w_i}})^2+(\sqrt h_i - {\sqrt{\hat h_i}})^2]\\<br>+\sum_{i=1}^{S^2} \sum_{j=1}^{B}<br>               [ 1_{ij}^{obj}(C_i - \hat {C_i})^2<br>+\lambda_{noobj} 1_{ij}^{noobj}(C_i - \hat {C_i})^2 ]    \\<br>+\sum_{i=1}^{S^2}1_i^{obj} \sum_{c\in classes} (p_i(c) - \hat p_i(c))^2<br>$$<br>其中：</p>
<p>$1_i^{obj}$：当<em>i</em>th网格中包含目标取1，否则取0。就是说当一个网格中存在目标时，才计算分类误差。</p>
<p>$1_{ij}^{obj}$：当<em>i</em>th网格的<em>j</em>th边框负责预测这个目标的时候，取值为1，否则为0。所以坐标误差中，实际有效的项共有P项，P是目标的个数。</p>
<p>$1_{ij}^{noobj}$：当<em>i</em>th网格的<em>j</em>th边框不负责预测任何目标的时候，取值为1，否则为0。对于置信度误差，所有的边框都计算，但不负责预测的边框加入系数$\lambda_{boobj}$。</p>
</div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://github.com/zixianzheng.github.io.git"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Kategorien</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux相关/">Linux相关</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/图形图像处理/">图形图像处理</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/object-detection/" style="font-size: 15px;">object detection</a> <a href="/tags/R-CNN/" style="font-size: 15px;">R-CNN</a> <a href="/tags/obejct-detection/" style="font-size: 15px;">obejct detection</a> <a href="/tags/DeepLearning/" style="font-size: 15px;">DeepLearning</a> <a href="/tags/VGG/" style="font-size: 15px;">VGG</a> <a href="/tags/Ubuntu14-04/" style="font-size: 15px;">Ubuntu14.04</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/GPU/" style="font-size: 15px;">GPU</a> <a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Letzte</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/05/02/YOLO/">YOLO</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/29/Faster R-CNN/">Faster R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/27/Fast R-CNN/">Fast R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/26/Ubuntu14.04下，Anaconda安装Tensorflow1.4.0-GPU/">Ubuntu14.04下，Anaconda安装Tensorflow1.4.0-GPU</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/25/常用Linux命令/">常用Linux命令</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/20/Bounding-box Regression/">Bounding-box Regression</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/20/R-CNN/">R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/15/VGGNet/">VGGNet</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="https://github.com/zixianZheng" title="github" target="_blank">github</a><ul></ul><a href="https://weibo.com/u/3700586755" title="weibo" target="_blank">weibo</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">噢噢噢噢奥利.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>
<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="专注于机器学习、深度学习、图形图像处理。"><title>Faster R-CNN | 噢噢噢噢奥利</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Faster R-CNN</h1><a id="logo" href="/.">噢噢噢噢奥利</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Faster R-CNN</h1><div class="post-content"><p>&emsp;&emsp;在Fast R-CNN的基础上，提出了Region Poposal Network，PRN与detection network共享卷积参数，实现了实时的目标检测。<a id="more"></a>RPN是一个全卷积神经网络，具有两个输出：目标候选框、目标是foreground和background的分值。RPN可以端到端的训练，产生高质量的候选框。</p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>&emsp;&emsp;这里先介绍一下Faster R-CNN整体过程：<br><img src="https://i.loli.net/2018/05/07/5aefc753424d1.png" alt="FasterRCNN-1.png"><br>&emsp;&emsp;1）原始图片送入CNN做特征提取得到feature maps，一般会把最后一个池化层去掉，这个CNN即是RPN和detection network共享的网络结构。<br>&emsp;&emsp;2）把feature maps送入RPN，得到候选框。<br>&emsp;&emsp;3）把feature maps和region proposals送入RoI Pooling，经过全连接网络，然后分别送入分类器和回归器（这里和Fast R-CNN是完全相同的）。</p>
<h2 id="2-Region-Proposal-Networks"><a href="#2-Region-Proposal-Networks" class="headerlink" title="2. Region Proposal Networks"></a>2. Region Proposal Networks</h2><p>&emsp;&emsp;一个RPN可以输入任意大小的图片，输出一系列矩形候选框，每个候选框都有前景和背景的分值。这里的输入图片就是特征图，任何尺寸都可以。</p>
<p>&emsp;&emsp;对于一个输入，假设是$W\times H\times C$的特征图，使用$n\times n$的卷积核进行运算（需要保证特征图大小不变，所以n=3，stride=1时，设定padding=1）。这里卷积核的个数就是下文低维向量的长度，256-d for ZF，512-d for VGG。卷积层输出$W\times H \times 256$（假设是ZF网络）的特征图，这里实际上并不看成特征图，而是看成<strong>在输入的每个sliding position上，都提取了一个256-d的特征向量</strong>，每个sliding position有$3\times 3$的有效域。</p>
<p>&emsp;&emsp;文中是说用一个”小网络“在输入上滑动，这个全连接型的小网络连接了输入的$n\times n$的窗口，这个小网络把每一个窗口都映射到了一个低维向量。小网络参数$n\times n \times C \times 256$。具体看下图，sliding window到256-d就是这个小网络做的事情：</p>
<p>&emsp;&emsp;现在，在输入特征图的每个位置上，我们都得到了一个256-d的特征向量，把这个向量分别送入两个全连接网络：1）box-regression layer，回归，边框精修；2）box-classification layer，边框分类。当然，这两个全连接网络可以看看成$1\times 1$的卷积核在上一个特征图上滑动。</p>
<h3 id="2-1-Anchors"><a href="#2-1-Anchors" class="headerlink" title="2.1 Anchors"></a>2.1 Anchors</h3><p>&emsp;&emsp;在feature map的每个点上，回归器预测出k个偏移，<strong>feature map上的点，映射回原图中的某个点，原图中的一个”某个点“对应k个anchor，就是说在feature map上的每个点都配备了k个anchor，偏移是指在anchor上的偏移</strong>，所以边框回归网络有4k个输出值。而边框分类层有2k的输出值，估计了这个点对应的k个anchor是前景和背景的概率（object/not-object的概率）。</p>
<p>&emsp;&emsp;作者是如何设置anchor的呢？ 作者在特征图的每个sliding position上，对应了原图中k=9种不同的anchor，包含了3种尺度、3种比例，如上图所示。对于$W\times H$大小的特征图，总计有$WHk$个anchor。通过这些不同尺度、比例的anchor，就在模型中引入了多尺度方法和平移不变性。</p>
<h3 id="2-2-RPN的损失函数"><a href="#2-2-RPN的损失函数" class="headerlink" title="2.2 RPN的损失函数"></a>2.2 RPN的损失函数</h3><p>&emsp;&emsp;<strong>训练样本如何标记？</strong>RPN中的分类器只有两种类别：是或不是目标。正例：1）与某个ground-ttuth的IoU最高的anchor为正例。2）与任何ground-truth的IoU&gt;0.7的anchor为正例。负例子：与所有ground-truth的IoU都小于0.3的anchor为负例。其他的anchors被抛弃掉。</p>
<p>&emsp;&emsp;<strong>如何定义RPN的损失函数？</strong>借鉴了Fast R-CNN中的multi-task loss：</p>
<h3 id="2-3-优化"><a href="#2-3-优化" class="headerlink" title="2.3 优化"></a>2.3 优化</h3><p>&emsp;&emsp;原始图片被缩放之后，送入CNN做特征提取，RPN可以看成全卷积神经网络，因此可以用BP+SGD进行端到端的训练。作者一次迭代只用一张图，一张图只取mini-batch=256个anchors，正负样本1：1，如果正例不足128个，用负例补充。</p>
<p>&emsp;&emsp;RPN新的层用高斯分布初始化，CNN特征提取的参数用监督预训练。其他具体的参数就不讲了。</p>
<h3 id="2-4-如何训练RPN和Fast-R-CNN两个网络"><a href="#2-4-如何训练RPN和Fast-R-CNN两个网络" class="headerlink" title="2.4 如何训练RPN和Fast R-CNN两个网络"></a>2.4 如何训练RPN和Fast R-CNN两个网络</h3><p>&emsp;&emsp;明确两点：首先要保证<strong>卷积特征共享</strong>，然后Fast R-CNN的训练依赖于<strong>确定</strong>的候选框，要是用mulit task loss优化一个损失函数，学习Fast R-CNN的时候RPN改变了，不知道能不能收敛。作者提出了”四步训练法”，这个方法通过交替式的优化来学习共享参数：</p>
<ol>
<li>训练RPN，方法如2.3中描述的一样：ImageNet做预训练，然后端到端到fine-tuning。</li>
<li>训练Fast R-CNN：ImageNet-pre-trained model做初始化，训练Fast R-CNN，候选框使用1中的RPN提取。</li>
<li>固定共享的参数，只更新RPN独有的参数。</li>
<li>固定共享的参数，固定RPN的独有的参数，只更新Fast R-CNN独有的部分（全连接那部分）。</li>
</ol>
<h3 id="2-5-实现细节"><a href="#2-5-实现细节" class="headerlink" title="2.5 实现细节"></a>2.5 实现细节</h3><p>&emsp;&emsp;我们需要把图片缩放到某个尺度送入Faster R-CNN，尝试缩放到多个尺度也许能够提高准确率，但是速度性能会损失。同样的，卷积使用更小的步长会提高准确率，损失速度。</p>
<p>&emsp;&emsp;对于超出图片边界的anchors：训练的时候，全部抛弃；测试的时候，把anchor截取到边界。</p>
<p>&emsp;&emsp;RPN会产生大量IoU很高的候选区域，通过非极大值抑制NMS，去除冗余的候选框，NMS时，IoU设定为0.7，基本上只剩下2000的候选框。在NMS之后，使用top-K的候选框做object detection。训练的时候K=2000，测试用另一个值。</p>
</div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://github.com/zixianzheng.github.io.git"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux相关/">Linux相关</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/图形图像处理/">图形图像处理</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/R-CNN/" style="font-size: 15px;">R-CNN</a> <a href="/tags/object-detection/" style="font-size: 15px;">object detection</a> <a href="/tags/obejct-detection/" style="font-size: 15px;">obejct detection</a> <a href="/tags/DeepLearning/" style="font-size: 15px;">DeepLearning</a> <a href="/tags/VGG/" style="font-size: 15px;">VGG</a> <a href="/tags/Ubuntu14-04/" style="font-size: 15px;">Ubuntu14.04</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/GPU/" style="font-size: 15px;">GPU</a> <a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/05/02/YOLO/">YOLO</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/29/Faster R-CNN/">Faster R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/27/Fast R-CNN/">Fast R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/26/Ubuntu14.04下，Anaconda安装Tensorflow1.4.0-GPU/">Ubuntu14.04下，Anaconda安装Tensorflow1.4.0-GPU</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/25/常用Linux命令/">常用Linux命令</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/24/Bounding-box Regression/">Bounding-box Regression</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/20/R-CNN/">R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/15/VGGNet/">VGGNet</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://github.com/zixianZheng" title="github" target="_blank">github</a><ul></ul><a href="https://weibo.com/u/3700586755" title="weibo" target="_blank">weibo</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">噢噢噢噢奥利.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>